# Expectation-Maximization via Pretext-Invariant Representations<br>
<img alt="GitHub top language" src="https://img.shields.io/github/languages/top/Leminhbinh0209/Empir?style=for-the-badge" height="25"  onmouseover="this.height='60'" onmouseout="this.height='25'" ><img alt="GitHub last commit" src="https://img.shields.io/github/last-commit/Leminhbinh0209/Empir?style=for-the-badge" height="25"><img alt="GitHub repo size" src="https://img.shields.io/github/repo-size/Leminhbinh0209/Empir?style=for-the-badge" height="25">
<br />
_IEEE Access 2023_
<br>


## Overview of our framework
<p align="center">
    <img src="https://i.ibb.co/JKkg8Zv/arc-empir3.gif"  width="800" alt="overall pipeline">
<p>

---
## Requirements
* torch
* torchvision
* tqdm
* einops
* wandb
* pytorch-lightning
* lightning-bolts
* torchmetrics
* scipy
* timm
---

## Installation

Please refer to [solo-learn](https://github.com/vturrisi/solo-learn) repository.

---
## Training

Please refer to `bash_files` folders. We provide bash files for both `CIFAR` and `ImageNet` datasets.

#
*Star if you find it useful.* ‚≠ê
